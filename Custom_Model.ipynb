{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "mkdir dataset\n"
      ],
      "metadata": {
        "id": "0o8p8C4SL-2c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh-Dw1ScOOL0",
        "outputId": "abb3e747-760b-4c14-e5f7-310848097d0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv test validation"
      ],
      "metadata": {
        "id": "8H0QaCaOOu3W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class FaceEmotionDetector:\n",
        "    def __init__(self):\n",
        "        # Define emotion labels\n",
        "        self.emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "        self.img_size = 48  # FER dataset uses 48x48 grayscale images\n",
        "        self.model = None\n",
        "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build the CNN model architecture\"\"\"\n",
        "        model = Sequential()\n",
        "\n",
        "        # First convolution block\n",
        "        model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(self.img_size, self.img_size, 1)))\n",
        "        model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        # Second convolution block\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        # Third convolution block\n",
        "        model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        # Fully connected layers\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(len(self.emotions), activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.0005),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, train_dir, validation_dir, epochs=50, batch_size=64):\n",
        "        \"\"\"Train the model using data from the given directories\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Create data generators for training and validation\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,\n",
        "            target_size=(self.img_size, self.img_size),\n",
        "            batch_size=batch_size,\n",
        "            color_mode='grayscale',\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        validation_generator = validation_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(self.img_size, self.img_size),\n",
        "            batch_size=batch_size,\n",
        "            color_mode='grayscale',\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        # Set up callbacks\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            \"emotion_model_best.h5\",\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1,\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        )\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            min_lr=0.00001\n",
        "        )\n",
        "\n",
        "        callbacks = [checkpoint, early_stopping, reduce_lr]\n",
        "\n",
        "        # Train the model\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            steps_per_epoch=train_generator.samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=validation_generator.samples // batch_size,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # Save the model\n",
        "        self.model.save('emotion_model.h5')\n",
        "\n",
        "        return history\n",
        "\n",
        "    def load_trained_model(self, model_path):\n",
        "        \"\"\"Load a trained model from disk\"\"\"\n",
        "        self.model = load_model(model_path)\n",
        "\n",
        "    def detect_emotion(self, image):\n",
        "        \"\"\"\n",
        "        Detect faces in the image and predict their emotions\n",
        "\n",
        "        Args:\n",
        "            image: numpy array of the image (BGR format from OpenCV)\n",
        "\n",
        "        Returns:\n",
        "            List of tuples with (face_box, emotion, confidence)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained or loaded\")\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces\n",
        "        faces = self.face_cascade.detectMultiScale(\n",
        "            gray,\n",
        "            scaleFactor=1.1,\n",
        "            minNeighbors=5,\n",
        "            minSize=(30, 30)\n",
        "        )\n",
        "\n",
        "        result = []\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Extract the face ROI\n",
        "            roi_gray = gray[y:y+h, x:x+h]\n",
        "\n",
        "            # Resize to expected size\n",
        "            roi_gray = cv2.resize(roi_gray, (self.img_size, self.img_size))\n",
        "\n",
        "            # Normalize and reshape for model input\n",
        "            roi = roi_gray.astype('float') / 255.0\n",
        "            roi = np.expand_dims(roi, axis=0)\n",
        "            roi = np.expand_dims(roi, axis=-1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(roi, verbose=0)[0]\n",
        "\n",
        "            # Get max confidence emotion\n",
        "            emotion_idx = np.argmax(prediction)\n",
        "            emotion = self.emotions[emotion_idx]\n",
        "            confidence = prediction[emotion_idx]\n",
        "\n",
        "            result.append(((x, y, w, h), emotion, confidence))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def visualize_results(self, image, results):\n",
        "        \"\"\"\n",
        "        Draw the results on the image\n",
        "\n",
        "        Args:\n",
        "            image: Original image\n",
        "            results: List of (face_box, emotion, confidence) tuples\n",
        "\n",
        "        Returns:\n",
        "            Image with annotated faces and emotions\n",
        "        \"\"\"\n",
        "        output = image.copy()\n",
        "\n",
        "        for (x, y, w, h), emotion, confidence in results:\n",
        "            # Draw rectangle around face\n",
        "            cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "            # Add text with emotion and confidence\n",
        "            text = f\"{emotion}: {confidence:.2f}\"\n",
        "            y_offset = y - 10 if y - 10 > 10 else y + h + 10\n",
        "            cv2.putText(output, text, (x, y_offset),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def demo_with_webcam():\n",
        "    \"\"\"Run a demo using webcam feed\"\"\"\n",
        "    detector = FaceEmotionDetector()\n",
        "\n",
        "    # Load pre-trained model (assuming it exists)\n",
        "    try:\n",
        "        detector.load_trained_model('emotion_model_best.h5')\n",
        "        print(\"Model loaded successfully\")\n",
        "    except:\n",
        "        print(\"Pre-trained model not found. Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Open webcam\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Could not open webcam\")\n",
        "        return\n",
        "\n",
        "    print(\"Press 'q' to quit\")\n",
        "\n",
        "    while True:\n",
        "        # Read frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"Failed to grab frame\")\n",
        "            break\n",
        "\n",
        "        # Detect emotions\n",
        "        results = detector.detect_emotion(frame)\n",
        "\n",
        "        # Visualize results\n",
        "        output_frame = detector.visualize_results(frame, results)\n",
        "\n",
        "        # Display result\n",
        "        cv2.imshow('Emotion Detection', output_frame)\n",
        "\n",
        "        # Break on 'q' key\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def download_fer_dataset():\n",
        "    \"\"\"\n",
        "    Instructions to download and prepare the FER2013 dataset\n",
        "\n",
        "    The FER2013 dataset is commonly used for facial emotion recognition\n",
        "    and contains ~35K 48x48 grayscale images of faces with 7 emotion categories.\n",
        "    \"\"\"\n",
        "    print(\"To train this model, you need the FER2013 dataset.\")\n",
        "    print(\"You can download it from Kaggle: https://www.kaggle.com/datasets/msambare/fer2013\")\n",
        "    print(\"\\nAfter downloading, organize the dataset as follows:\")\n",
        "    print(\"dataset/\")\n",
        "    print(\"├── train/\")\n",
        "    print(\"│   ├── angry/\")\n",
        "    print(\"│   ├── disgust/\")\n",
        "    print(\"│   ├── fear/\")\n",
        "    print(\"│   ├── happy/\")\n",
        "    print(\"│   ├── sad/\")\n",
        "    print(\"│   ├── surprise/\")\n",
        "    print(\"│   └── neutral/\")\n",
        "    print(\"└── validation/\")\n",
        "    print(\"    ├── angry/\")\n",
        "    print(\"    ├── disgust/\")\n",
        "    print(\"    ├── fear/\")\n",
        "    print(\"    ├── happy/\")\n",
        "    print(\"    ├── sad/\")\n",
        "    print(\"    ├── surprise/\")\n",
        "    print(\"    └── neutral/\")\n",
        "    print(\"\\nThen you can train the model using:\")\n",
        "    print(\"detector = FaceEmotionDetector()\")\n",
        "    print(\"detector.train('dataset/train', 'dataset/validation')\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose what to do\n",
        "    print(\"1. Download and prepare dataset\")\n",
        "    print(\"2. Run webcam demo\")\n",
        "    choice = input(\"Enter your choice (1/2): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        download_fer_dataset()\n",
        "    elif choice == '2':\n",
        "        demo_with_webcam()\n",
        "    else:\n",
        "        print(\"Invalid choice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sUO2kyaO3_N",
        "outputId": "06586cdc-ba58-4f66-d12d-8a8230806b0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Download and prepare dataset\n",
            "2. Run webcam demo\n",
            "Enter your choice (1/2): 1\n",
            "To train this model, you need the FER2013 dataset.\n",
            "You can download it from Kaggle: https://www.kaggle.com/datasets/msambare/fer2013\n",
            "\n",
            "After downloading, organize the dataset as follows:\n",
            "dataset/\n",
            "├── train/\n",
            "│   ├── angry/\n",
            "│   ├── disgust/\n",
            "│   ├── fear/\n",
            "│   ├── happy/\n",
            "│   ├── sad/\n",
            "│   ├── surprise/\n",
            "│   └── neutral/\n",
            "└── validation/\n",
            "    ├── angry/\n",
            "    ├── disgust/\n",
            "    ├── fear/\n",
            "    ├── happy/\n",
            "    ├── sad/\n",
            "    ├── surprise/\n",
            "    └── neutral/\n",
            "\n",
            "Then you can train the model using:\n",
            "detector = FaceEmotionDetector()\n",
            "detector.train('dataset/train', 'dataset/validation')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class LightweightFaceEmotionDetector:\n",
        "    def __init__(self):\n",
        "        # Define emotion labels\n",
        "        self.emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "        self.img_size = 48  # FER dataset uses 48x48 grayscale images\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build the lightweight CNN model architecture\"\"\"\n",
        "        model = Sequential()\n",
        "\n",
        "        # First convolution block\n",
        "        model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(self.img_size, self.img_size, 1)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        # Second convolution block\n",
        "        model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        # Fully connected layers\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(len(self.emotions), activation='softmax'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.0005),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, train_dir, validation_dir, epochs=50, batch_size=64):\n",
        "        \"\"\"Train the model using data from the given directories\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Create data generators for training and validation\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,\n",
        "            target_size=(self.img_size, self.img_size),\n",
        "            batch_size=batch_size,\n",
        "            color_mode='grayscale',\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        validation_generator = validation_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(self.img_size, self.img_size),\n",
        "            batch_size=batch_size,\n",
        "            color_mode='grayscale',\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        # Set up callbacks\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            \"lightweight_emotion_model_best.h5\",\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1,\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        )\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=5,\n",
        "            verbose=1,\n",
        "            min_lr=0.00001\n",
        "        )\n",
        "\n",
        "        callbacks = [checkpoint, early_stopping, reduce_lr]\n",
        "\n",
        "        # Train the model\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            steps_per_epoch=train_generator.samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=validation_generator.samples // batch_size,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        # Save the model\n",
        "        self.model.save('lightweight_emotion_model.h5')\n",
        "\n",
        "        return history"
      ],
      "metadata": {
        "id": "CjCLZ3yiP_kV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your dataset\n",
        "train_dir = 'train'\n",
        "validation_dir = 'validation'\n",
        "\n",
        "# Initialize the detector\n",
        "detector = LightweightFaceEmotionDetector()\n",
        "\n",
        "# Train the model\n",
        "history = detector.train(train_dir, validation_dir, epochs=50, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQp09Ya3QBCV",
        "outputId": "bbef8472-79f9-46ca-f88b-80b627db2eda"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - accuracy: 0.2019 - loss: 2.3908\n",
            "Epoch 1: val_accuracy improved from -inf to 0.18025, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 399ms/step - accuracy: 0.2020 - loss: 2.3904 - val_accuracy: 0.1802 - val_loss: 8.8572 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 365ms/step - accuracy: 0.2500 - loss: 2.0104"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy did not improve from 0.18025\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.2500 - loss: 2.0104 - val_accuracy: 0.1802 - val_loss: 8.7225 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.2810 - loss: 1.8743\n",
            "Epoch 3: val_accuracy improved from 0.18025 to 0.35017, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 420ms/step - accuracy: 0.2810 - loss: 1.8742 - val_accuracy: 0.3502 - val_loss: 1.6404 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 284ms/step - accuracy: 0.2812 - loss: 1.6359\n",
            "Epoch 4: val_accuracy improved from 0.35017 to 0.35184, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.2812 - loss: 1.6359 - val_accuracy: 0.3518 - val_loss: 1.6338 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.3344 - loss: 1.7039\n",
            "Epoch 5: val_accuracy improved from 0.35184 to 0.39565, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 367ms/step - accuracy: 0.3344 - loss: 1.7038 - val_accuracy: 0.3956 - val_loss: 1.5119 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 274ms/step - accuracy: 0.3125 - loss: 1.7234\n",
            "Epoch 6: val_accuracy did not improve from 0.39565\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3125 - loss: 1.7234 - val_accuracy: 0.3841 - val_loss: 1.5256 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.3654 - loss: 1.6254\n",
            "Epoch 7: val_accuracy improved from 0.39565 to 0.41867, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 389ms/step - accuracy: 0.3654 - loss: 1.6253 - val_accuracy: 0.4187 - val_loss: 1.5054 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 300ms/step - accuracy: 0.4062 - loss: 1.5767\n",
            "Epoch 8: val_accuracy improved from 0.41867 to 0.42090, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.4062 - loss: 1.5767 - val_accuracy: 0.4209 - val_loss: 1.5026 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.3871 - loss: 1.5760\n",
            "Epoch 9: val_accuracy did not improve from 0.42090\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 361ms/step - accuracy: 0.3871 - loss: 1.5760 - val_accuracy: 0.4134 - val_loss: 1.5202 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 270ms/step - accuracy: 0.3906 - loss: 1.4700\n",
            "Epoch 10: val_accuracy did not improve from 0.42090\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3906 - loss: 1.4700 - val_accuracy: 0.4104 - val_loss: 1.5133 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.3956 - loss: 1.5518\n",
            "Epoch 11: val_accuracy improved from 0.42090 to 0.43597, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 381ms/step - accuracy: 0.3956 - loss: 1.5518 - val_accuracy: 0.4360 - val_loss: 1.4540 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 292ms/step - accuracy: 0.4531 - loss: 1.3541\n",
            "Epoch 12: val_accuracy improved from 0.43597 to 0.43890, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4531 - loss: 1.3541 - val_accuracy: 0.4389 - val_loss: 1.4448 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4087 - loss: 1.5321\n",
            "Epoch 13: val_accuracy did not improve from 0.43890\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 450ms/step - accuracy: 0.4087 - loss: 1.5321 - val_accuracy: 0.4347 - val_loss: 1.4791 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 285ms/step - accuracy: 0.3750 - loss: 1.5028\n",
            "Epoch 14: val_accuracy improved from 0.43890 to 0.44127, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3750 - loss: 1.5028 - val_accuracy: 0.4413 - val_loss: 1.4639 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.4186 - loss: 1.5059\n",
            "Epoch 15: val_accuracy improved from 0.44127 to 0.45257, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 451ms/step - accuracy: 0.4186 - loss: 1.5059 - val_accuracy: 0.4526 - val_loss: 1.4541 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 277ms/step - accuracy: 0.3438 - loss: 1.6292\n",
            "Epoch 16: val_accuracy did not improve from 0.45257\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3438 - loss: 1.6292 - val_accuracy: 0.4520 - val_loss: 1.4508 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.4291 - loss: 1.4828\n",
            "Epoch 17: val_accuracy improved from 0.45257 to 0.46805, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 451ms/step - accuracy: 0.4291 - loss: 1.4828 - val_accuracy: 0.4681 - val_loss: 1.4055 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 285ms/step - accuracy: 0.5000 - loss: 1.4696\n",
            "Epoch 18: val_accuracy did not improve from 0.46805\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.5000 - loss: 1.4696 - val_accuracy: 0.4665 - val_loss: 1.4059 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.4331 - loss: 1.4733\n",
            "Epoch 19: val_accuracy did not improve from 0.46805\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 451ms/step - accuracy: 0.4331 - loss: 1.4733 - val_accuracy: 0.4538 - val_loss: 1.4293 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 286ms/step - accuracy: 0.4219 - loss: 1.4148\n",
            "Epoch 20: val_accuracy did not improve from 0.46805\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.4219 - loss: 1.4148 - val_accuracy: 0.4611 - val_loss: 1.4103 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.4392 - loss: 1.4670\n",
            "Epoch 21: val_accuracy improved from 0.46805 to 0.48675, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 454ms/step - accuracy: 0.4392 - loss: 1.4669 - val_accuracy: 0.4867 - val_loss: 1.4258 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 305ms/step - accuracy: 0.4375 - loss: 1.3162\n",
            "Epoch 22: val_accuracy did not improve from 0.48675\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4375 - loss: 1.3162 - val_accuracy: 0.4837 - val_loss: 1.4743 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - accuracy: 0.4490 - loss: 1.4356\n",
            "Epoch 23: val_accuracy improved from 0.48675 to 0.51214, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 473ms/step - accuracy: 0.4490 - loss: 1.4356 - val_accuracy: 0.5121 - val_loss: 1.2756 - learning_rate: 1.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 434ms/step - accuracy: 0.3438 - loss: 1.5848\n",
            "Epoch 24: val_accuracy improved from 0.51214 to 0.51256, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.3438 - loss: 1.5848 - val_accuracy: 0.5126 - val_loss: 1.2758 - learning_rate: 1.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.4550 - loss: 1.4166\n",
            "Epoch 25: val_accuracy did not improve from 0.51256\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 371ms/step - accuracy: 0.4550 - loss: 1.4166 - val_accuracy: 0.4902 - val_loss: 1.3071 - learning_rate: 1.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 321ms/step - accuracy: 0.5469 - loss: 1.3318\n",
            "Epoch 26: val_accuracy did not improve from 0.51256\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.5469 - loss: 1.3318 - val_accuracy: 0.4907 - val_loss: 1.3063 - learning_rate: 1.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4602 - loss: 1.4147\n",
            "Epoch 27: val_accuracy improved from 0.51256 to 0.52037, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 372ms/step - accuracy: 0.4602 - loss: 1.4147 - val_accuracy: 0.5204 - val_loss: 1.2553 - learning_rate: 1.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 290ms/step - accuracy: 0.5312 - loss: 1.2577\n",
            "Epoch 28: val_accuracy improved from 0.52037 to 0.52079, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.5312 - loss: 1.2577 - val_accuracy: 0.5208 - val_loss: 1.2540 - learning_rate: 1.0000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.4631 - loss: 1.4042\n",
            "Epoch 29: val_accuracy did not improve from 0.52079\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 371ms/step - accuracy: 0.4631 - loss: 1.4042 - val_accuracy: 0.5075 - val_loss: 1.2791 - learning_rate: 1.0000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 275ms/step - accuracy: 0.3125 - loss: 1.7100\n",
            "Epoch 30: val_accuracy did not improve from 0.52079\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.3125 - loss: 1.7100 - val_accuracy: 0.5075 - val_loss: 1.2788 - learning_rate: 1.0000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step - accuracy: 0.4648 - loss: 1.4057\n",
            "Epoch 31: val_accuracy did not improve from 0.52079\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 448ms/step - accuracy: 0.4648 - loss: 1.4058 - val_accuracy: 0.5152 - val_loss: 1.2753 - learning_rate: 1.0000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 291ms/step - accuracy: 0.5781 - loss: 1.1164\n",
            "Epoch 32: val_accuracy did not improve from 0.52079\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.5781 - loss: 1.1164 - val_accuracy: 0.5159 - val_loss: 1.2741 - learning_rate: 1.0000e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.4610 - loss: 1.4041\n",
            "Epoch 33: val_accuracy did not improve from 0.52079\n",
            "\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 452ms/step - accuracy: 0.4610 - loss: 1.4041 - val_accuracy: 0.5184 - val_loss: 1.2689 - learning_rate: 1.0000e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 274ms/step - accuracy: 0.3594 - loss: 1.6240\n",
            "Epoch 34: val_accuracy did not improve from 0.52079\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3594 - loss: 1.6240 - val_accuracy: 0.5188 - val_loss: 1.2687 - learning_rate: 2.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - accuracy: 0.4693 - loss: 1.3963\n",
            "Epoch 35: val_accuracy improved from 0.52079 to 0.52441, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 372ms/step - accuracy: 0.4693 - loss: 1.3963 - val_accuracy: 0.5244 - val_loss: 1.2508 - learning_rate: 2.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 276ms/step - accuracy: 0.5156 - loss: 1.3256\n",
            "Epoch 36: val_accuracy improved from 0.52441 to 0.52455, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.5156 - loss: 1.3256 - val_accuracy: 0.5246 - val_loss: 1.2506 - learning_rate: 2.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 0.4674 - loss: 1.3950\n",
            "Epoch 37: val_accuracy did not improve from 0.52455\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 371ms/step - accuracy: 0.4675 - loss: 1.3950 - val_accuracy: 0.5246 - val_loss: 1.2499 - learning_rate: 2.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 297ms/step - accuracy: 0.4062 - loss: 1.6421\n",
            "Epoch 38: val_accuracy did not improve from 0.52455\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4062 - loss: 1.6421 - val_accuracy: 0.5246 - val_loss: 1.2504 - learning_rate: 2.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step - accuracy: 0.4737 - loss: 1.3881\n",
            "Epoch 39: val_accuracy improved from 0.52455 to 0.52637, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 368ms/step - accuracy: 0.4737 - loss: 1.3881 - val_accuracy: 0.5264 - val_loss: 1.2425 - learning_rate: 2.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 482ms/step - accuracy: 0.4844 - loss: 1.3739\n",
            "Epoch 40: val_accuracy improved from 0.52637 to 0.52734, saving model to lightweight_emotion_model_best.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.4844 - loss: 1.3739 - val_accuracy: 0.5273 - val_loss: 1.2410 - learning_rate: 2.0000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4681 - loss: 1.3950\n",
            "Epoch 41: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 373ms/step - accuracy: 0.4681 - loss: 1.3950 - val_accuracy: 0.5236 - val_loss: 1.2558 - learning_rate: 2.0000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 277ms/step - accuracy: 0.4375 - loss: 1.4054\n",
            "Epoch 42: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4375 - loss: 1.4054 - val_accuracy: 0.5236 - val_loss: 1.2565 - learning_rate: 2.0000e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.4726 - loss: 1.3865\n",
            "Epoch 43: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 382ms/step - accuracy: 0.4726 - loss: 1.3865 - val_accuracy: 0.5239 - val_loss: 1.2510 - learning_rate: 2.0000e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 298ms/step - accuracy: 0.2656 - loss: 1.5889\n",
            "Epoch 44: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.2656 - loss: 1.5889 - val_accuracy: 0.5239 - val_loss: 1.2506 - learning_rate: 2.0000e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.4758 - loss: 1.3755\n",
            "Epoch 45: val_accuracy did not improve from 0.52734\n",
            "\n",
            "Epoch 45: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 402ms/step - accuracy: 0.4757 - loss: 1.3755 - val_accuracy: 0.5255 - val_loss: 1.2453 - learning_rate: 2.0000e-05\n",
            "Epoch 46/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 306ms/step - accuracy: 0.4688 - loss: 1.3291\n",
            "Epoch 46: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4688 - loss: 1.3291 - val_accuracy: 0.5248 - val_loss: 1.2453 - learning_rate: 1.0000e-05\n",
            "Epoch 47/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - accuracy: 0.4710 - loss: 1.3871\n",
            "Epoch 47: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 422ms/step - accuracy: 0.4710 - loss: 1.3871 - val_accuracy: 0.5240 - val_loss: 1.2458 - learning_rate: 1.0000e-05\n",
            "Epoch 48/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 306ms/step - accuracy: 0.4375 - loss: 1.3661\n",
            "Epoch 48: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - accuracy: 0.4375 - loss: 1.3661 - val_accuracy: 0.5244 - val_loss: 1.2457 - learning_rate: 1.0000e-05\n",
            "Epoch 49/50\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.4715 - loss: 1.3839\n",
            "Epoch 49: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 453ms/step - accuracy: 0.4715 - loss: 1.3839 - val_accuracy: 0.5251 - val_loss: 1.2512 - learning_rate: 1.0000e-05\n",
            "Epoch 50/50\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 296ms/step - accuracy: 0.5000 - loss: 1.3018\n",
            "Epoch 50: val_accuracy did not improve from 0.52734\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.5000 - loss: 1.3018 - val_accuracy: 0.5251 - val_loss: 1.2513 - learning_rate: 1.0000e-05\n",
            "Epoch 50: early stopping\n",
            "Restoring model weights from the end of the best epoch: 40.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_with_webcam():\n",
        "    \"\"\"Run a demo using webcam feed\"\"\"\n",
        "    detector = LightweightFaceEmotionDetector()\n",
        "\n",
        "    # Load pre-trained model (assuming it exists)\n",
        "    try:\n",
        "        detector.load_trained_model('lightweight_emotion_model_best.h5')\n",
        "        print(\"Model loaded successfully\")\n",
        "    except:\n",
        "        print(\"Pre-trained model not found. Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Open webcam\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Could not open webcam\")\n",
        "        return\n",
        "\n",
        "    print(\"Press 'q' to quit\")\n",
        "\n",
        "    while True:\n",
        "        # Read frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"Failed to grab frame\")\n",
        "            break\n",
        "\n",
        "        # Detect emotions\n",
        "        results = detector.detect_emotion(frame)\n",
        "\n",
        "        # Visualize results\n",
        "        output_frame = detector.visualize_results(frame, results)\n",
        "\n",
        "        # Display result\n",
        "        cv2.imshow('Emotion Detection', output_frame)\n",
        "\n",
        "        # Break on 'q' key\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "weWOZE5LlX-f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_with_webcam()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T53XLPpqlZVu",
        "outputId": "645bb55f-015b-4786-bf74-383c1beddabf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained model not found. Please train the model first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHFFlG1WQJ0F",
        "outputId": "834e0282-04e4-4732-86fb-e981c658f279"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lightweight_emotion_model_best.h5  lightweight_emotion_model.h5  \u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalidation\u001b[0m/\n"
          ]
        }
      ]
    }
  ]
}